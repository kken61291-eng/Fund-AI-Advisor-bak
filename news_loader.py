import akshare as ak
import json
import os
import time
import requests
import pandas as pd
from datetime import datetime
import hashlib
import pytz
import re
from bs4 import BeautifulSoup

# --- Selenium æ¨¡å— (æ¨¡æ‹Ÿæµè§ˆå™¨ä¸“ç”¨) ---
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- é…ç½® ---
DATA_DIR = "data_news"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

def get_beijing_time():
    return datetime.now(pytz.timezone('Asia/Shanghai'))

def get_today_str():
    return get_beijing_time().strftime("%Y-%m-%d")

def generate_news_id(item):
    raw = f"{item.get('time','')}{item.get('title','')}"
    return hashlib.md5(raw.encode('utf-8')).hexdigest()

def clean_time_str(t_str):
    if not t_str: return ""
    try:
        if len(str(t_str)) == 10: 
             return datetime.fromtimestamp(int(t_str)).strftime("%Y-%m-%d %H:%M:%S")
        if len(str(t_str)) > 19:
            return str(t_str)[:19]
        return str(t_str)
    except:
        return str(t_str)

# ==========================================
# 1. ä¸œè´¢æŠ“å– (åŒä¿é™©æ¨¡å¼ - ä¿®å¤è§£æ)
# ==========================================
def fetch_eastmoney_direct():
    """
    [Plan B] ç›´è¿ä¸œè´¢æ¥å£ï¼Œä½¿ç”¨å­—ç¬¦ä¸²æˆªå–æ³•è§£æ
    """
    items = []
    try:
        print("   - [Plan B] å¯åŠ¨ä¸œè´¢ç›´è¿æ¨¡å¼ (Direct API)...")
        # ä¸œè´¢ 7x24 å¿«è®¯æ¥å£
        url = "https://newsapi.eastmoney.com/kuaixun/v1/getlist_102_ajaxResult_50_1_.html"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://kuaixun.eastmoney.com/"
        }
        # å¢åŠ è¶…æ—¶æ—¶é—´
        resp = requests.get(url, headers=headers, timeout=15)
        
        if resp.status_code == 200:
            text = resp.text
            
            # [æ ¸å¿ƒä¿®å¤] ä¸ç”¨æ­£åˆ™ï¼Œç›´æ¥æš´åŠ›æˆªå–ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹
            try:
                start_idx = text.find('{')
                end_idx = text.rfind('}')
                
                if start_idx != -1 and end_idx != -1:
                    json_str = text[start_idx : end_idx + 1]
                    data = json.loads(json_str)
                    news_list = data.get('LivesList', [])
                    
                    for news in news_list:
                        title = news.get('title', '').strip()
                        digest = news.get('digest', '').strip()
                        show_time = news.get('showtime', '') 
                        
                        # å†…å®¹å¤„ç†
                        content = digest if len(digest) > len(title) else title
                        
                        if not title: continue
                        
                        items.append({
                            "time": show_time,
                            "title": title,
                            "content": content,
                            "source": "EastMoney"
                        })
                    print(f"   - [Plan B] æˆåŠŸè§£æå¹¶è·å– {len(items)} æ¡æ•°æ®")
                else:
                    print("   - [Plan B] æœªæ‰¾åˆ° JSON ç»“æ„ ({} ä¸åŒ¹é…)")
            except Exception as parse_e:
                print(f"   - [Plan B] JSON è§£æå¼‚å¸¸: {parse_e}")
        else:
            print(f"   - [Plan B] HTTPè¯·æ±‚å¤±è´¥: {resp.status_code}")
            
    except Exception as e:
        print(f"   âŒ [Plan B] ä¸œè´¢ç›´è¿å¤±è´¥: {e}")
    return items

def fetch_eastmoney():
    items = []
    # --- å°è¯• Plan A: Akshare ---
    try:
        print("   - [Plan A] æ­£åœ¨æŠ“å–: ä¸œæ–¹è´¢å¯Œ (Akshare)...")
        df_em = ak.stock_telegraph_em()
        if df_em is not None and not df_em.empty:
            for _, row in df_em.iterrows():
                title = str(row.get('title', '')).strip()
                content = str(row.get('content', '')).strip()
                public_time = clean_time_str(row.get('public_time', ''))
                
                if not title or len(title) < 2: continue
                items.append({
                    "time": public_time,
                    "title": title,
                    "content": content,
                    "source": "EastMoney"
                })
            print(f"   - [Plan A] æˆåŠŸè·å– {len(items)} æ¡æ•°æ®")
            return items
    except AttributeError:
        print("   âš ï¸ Akshare ç‰ˆæœ¬ä¸å…¼å®¹ (AttributeError)ï¼Œåˆ‡æ¢è‡³ Plan B...")
    except Exception as e:
        print(f"   âš ï¸ Akshare è°ƒç”¨å‡ºé”™ï¼Œåˆ‡æ¢è‡³ Plan B...")

    # --- å¤±è´¥åˆ™æ‰§è¡Œ Plan B ---
    return fetch_eastmoney_direct()

# ==========================================
# 2. è´¢è”ç¤¾æŠ“å– (æµè§ˆå™¨æ¨¡å¼)
# ==========================================
def fetch_cls_selenium():
    items = []
    driver = None
    try:
        print("   - [Browser] æ­£åœ¨å¯åŠ¨ Chrome æŠ“å–: è´¢è”ç¤¾ (CLS)...")
        
        chrome_options = Options()
        chrome_options.add_argument("--headless") 
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36")

        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(60)
        
        url = "https://www.cls.cn/telegraph"
        driver.get(url)
        
        # ç­‰å¾…åŠ è½½
        try:
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.CLASS_NAME, "telegraph-list"))
            )
        except:
            print("   âš ï¸ ç­‰å¾…ç½‘é¡µåŠ è½½è¶…æ—¶ï¼Œå°è¯•ç›´æ¥è§£æ...")

        # æ¨¡æ‹Ÿæ»šåŠ¨
        driver.execute_script("window.scrollTo(0, 1000);")
        time.sleep(3) 

        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        
        nodes = soup.find_all("div", class_="telegraph-list-item")
        if not nodes:
            nodes = soup.select("div.telegraph-content-box")

        print(f"   - æ•è·åˆ° {len(nodes)} ä¸ªç½‘é¡µèŠ‚ç‚¹")

        current_date_prefix = get_beijing_time().strftime("%Y-%m-%d")

        for node in nodes:
            try:
                time_span = node.find("span", class_="telegraph-time")
                time_str = time_span.get_text().strip() if time_span else ""
                
                # [æ ¸å¿ƒä¿®å¤] å¢å¼ºæ—¥æœŸè¡¥å…¨é€»è¾‘
                # åªè¦æ˜¯ç±»ä¼¼ HH:MM (5ä½) æˆ– HH:MM:SS (8ä½) çš„çŸ­æ—¶é—´ï¼Œéƒ½è¡¥å…¨æ—¥æœŸ
                if len(time_str) < 10 and ":" in time_str:
                    # å¦‚æœåªæœ‰ 5 ä½ (13:16)ï¼Œè¡¥ç§’
                    if len(time_str) <= 5:
                        full_time = f"{current_date_prefix} {time_str}:00"
                    else:
                        # å¦åˆ™ç›´æ¥æ‹¼æ—¥æœŸ (13:16:49)
                        full_time = f"{current_date_prefix} {time_str}"
                else:
                    # å·²ç»æ˜¯å®Œæ•´æ—¶é—´æˆ–å…¶ä»–æ ¼å¼
                    full_time = time_str

                content_div = node.find("div", class_="telegraph-content")
                if not content_div:
                    content_div = node.find("div", class_="telegraph-detail")
                
                content_text = content_div.get_text().strip() if content_div else ""
                
                if content_text:
                    title = content_text[:40] + "..." if len(content_text) > 40 else content_text
                    
                    items.append({
                        "time": full_time,
                        "title": title,
                        "content": content_text,
                        "source": "CLS"
                    })
            except: continue

    except Exception as e:
        print(f"   âŒ è´¢è”ç¤¾(Selenium)æŠ“å–å¤±è´¥: {e}")
    finally:
        if driver:
            driver.quit()
    
    return items

# ==========================================
# ä¸»ç¨‹åº
# ==========================================
def fetch_and_save_news():
    today_date = get_today_str()
    print(f"ğŸ“¡ [NewsLoader] å¯åŠ¨æ··åˆæŠ“å– (Smart Mode) - {today_date}...")
    
    all_news_items = []

    # 1. ä¸œè´¢ (API + ç›´è¿å¤‡ä»½)
    em_items = fetch_eastmoney()
    all_news_items.extend(em_items)

    print(f"â³ ä¸œè´¢æŠ“å–å®Œæ¯•ï¼Œæ­£åœ¨ä¼‘çœ  50 ç§’ (é¿å…è¯·æ±‚è¿‡å¿«)...")
    time.sleep(50)

    # 2. è´¢è”ç¤¾ (Selenium)
    cls_items = fetch_cls_selenium()
    all_news_items.extend(cls_items)

    # 3. å…¥åº“
    if not all_news_items:
        print("âš ï¸ æœªè·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®")
        return

    today_file = os.path.join(DATA_DIR, f"news_{today_date}.jsonl")
    existing_ids = set()
    
    if os.path.exists(today_file):
        with open(today_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    saved_item = json.loads(line)
                    if 'id' in saved_item:
                        existing_ids.add(saved_item['id'])
                except: pass

    new_count = 0
    # é‡æ–°æ’åºï¼šç¡®ä¿æœ‰æ—¶é—´çš„æ’å‰é¢
    all_news_items.sort(key=lambda x: x['time'], reverse=True)

    with open(today_file, 'a', encoding='utf-8') as f:
        for item in all_news_items:
            item_id = generate_news_id(item)
            item['id'] = item_id
            
            if item_id not in existing_ids:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
                existing_ids.add(item_id)
                new_count += 1
    
    print(f"âœ… å…¥åº“å®Œæˆ: æ–°å¢ {new_count} æ¡ (EM:{len(em_items)} | CLS:{len(cls_items)})")

if __name__ == "__main__":
    fetch_and_save_news()
